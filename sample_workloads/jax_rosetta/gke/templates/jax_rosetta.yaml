{{ $timestamp := now | date "2006-01-02-150405" }}
{{ $jobidentifier := randAlphaNum 8 | lower }}
apiVersion: v1
kind: Service
metadata:
  name: "jax-job-{{$jobidentifier}}"
spec:
  clusterIP: None
  selector:
    job-name: "jax-job-{{$jobidentifier}}"
---
{{- $root := . -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: "jax-job-{{$jobidentifier}}"
  namespace: default
  labels:
    kueue.x-k8s.io/queue-name: a3-queue 
spec:
  suspend: false
  parallelism: {{ $root.Values.workload.nodes }}
  completions: {{ $root.Values.workload.nodes }}
  completionMode: Indexed
  template:
   metadata:
    annotations:
      kubectl.kubernetes.io/default-container: jax
   spec:
    hostNetwork: true
    dnsPolicy: ClusterFirstWithHostNet
    subdomain: "jax-job-{{$jobidentifier}}"
    restartPolicy: Never
    volumes:
    - name: nvidia-install-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: lib64
      hostPath:
        path: /lib64       
    - name: tcpx-nccl-plugin-volume
      emptyDir: {}    
    - name: tcpx-daemon-socket
      hostPath:
        path: /run/tcpx
    - name: workload-terminated-volume
      emptyDir: {}        
    - name: local-ssd
      hostPath:
        path: /mnt/stateful_partition/kube-ephemeral-ssd   
    # - name: cluster-file-store
    #   persistentVolumeClaim:
    #     claimName: cluster-sharedfs
    initContainers:
    {{ if eq $root.Values.tcpx.enabled "true" }}
    - name: tcpx-nccl-plugin-installer
      image: "{{$root.Values.tcpx.repository}}/{{$root.Values.tcpx.pluginVersion}}"
      imagePullPolicy: Always
      volumeMounts:
      - name: tcpx-nccl-plugin-volume
        mountPath: /var/lib/tcpx
      command:
        - /bin/sh
        - -c
        - |
          /scripts/container_entry.sh install --install-nccl
    {{ end }}  
    containers:
    {{ if eq $root.Values.tcpx.enabled "true" }}
    - name: tcpd-daemon
      image: "{{$root.Values.tcpx.repository}}/{{$root.Values.tcpx.daemonVersion}}"
      imagePullPolicy: Always
      command:
      - "bash"
      - "-c"
      - |
        /tcpgpudmarxd/build/app/tcpgpudmarxd --gpu_nic_preset a3vm --gpu_shmem_type fd --setup_param "--verbose 128 5 0" &
        while [ ! -e "/semaphore/workload_terminated" ]; do sleep 10; done
        pkill -e "^"tcpgpudmarxd || true
        sleep 15
      securityContext:
        privileged: true
      volumeMounts:
      - name: nvidia-install-dir-host
        mountPath: /usr/local/nvidia
      - name: tcpx-daemon-socket
        mountPath: /tmp
      - name: workload-terminated-volume
        mountPath: /semaphore
      env:
      - name: LD_LIBRARY_PATH
        value: /usr/local/nvidia/lib64
    {{ end }} 
    - name: jax
      image: "{{ $root.Values.workload.image }}"
      imagePullPolicy: Always
      env:
      - name: MASTER_ADDR
        value: "jax-job-{{$jobidentifier}}-0.jax-job-{{$jobidentifier}}.default.svc.cluster.local"
      - name: NNODES
        value: "{{ $root.Values.workload.nodes }}"
      - name: BASE_WORKSPACE_DIR
        value: "{{ $root.Values.workload.baseWorkspaceDir }}"
      - name: CONFIG
        value: "{{ $root.Values.workload.config }}"
      - name: OUTPUT_DIR
        value: "{{ $root.Values.workload.outputDir }}"


      {{ if eq $root.Values.tcpx.enabled "true" }}

       # The following NCCL settings should likely not be adjusted:
      - name: NCCL_SOCKET_IFNAME
        value: "eth0"
      - name: NCCL_CHECK_POINTERS
        value: "0"
      - name: NCCL_DYNAMIC_CHUNK_SIZE
        value: "524288"
      - name: NCCL_P2P_NET_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_PCI_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_NVL_CHUNKSIZE
        value: "1048576"
      - name: NCCL_CROSS_NIC
        value: "0"
      - name: NCCL_ALGO
        value: "Ring"
      - name: NCCL_PROTO
        value: "Simple"
      - name: NCCL_NET_GDR_LEVEL
        value: "PIX"
      - name: NCCL_P2P_PXN_LEVEL
        value: "0"
 
      {{- range $environment_variable := $root.Values.tcpx.ncclSettings }}
      - name: {{ $environment_variable.name }}
        value: "{{ $environment_variable.value }}"
      {{- end }}
    
      # The following TCPx settings should likely not be adjusted:
      - name: NCCL_GPUDIRECTTCPX_CTRL_DEV
        value: "eth0"
      - name: NCCL_GPUDIRECTTCPX_SOCKET_IFNAME
        value: "eth1,eth2,eth3,eth4"
      - name: NCCL_GPUDIRECTTCPX_TX_BINDINGS
        value: "eth1:8-21,112-125;eth2:8-21,112-125;eth3:60-73,164-177;eth4:60-73,164-177"
      - name: NCCL_GPUDIRECTTCPX_RX_BINDINGS
        value: "eth1:22-35,126-139;eth2:22-35,126-139;eth3:74-87,178-191;eth4:74-87,178-191"
      - name: NCCL_GPUDIRECTTCPX_PROGRAM_FLOW_STEERING_WAIT_MICROS
        value: "1000000"
      - name: NCCL_GPUDIRECTTCPX_FORCE_ACK
        value: "0"
      - name: NCCL_GPUDIRECTTCPX_TX_COMPLETION_NANOSLEEP
        value: "1000"     
    
      {{- range $environment_variable := $root.Values.tcpx.tcpxSettings }}
      - name: {{ $environment_variable.name }}
        value: "{{ $environment_variable.value }}"
      {{- end }}
    
      {{ end }} 
      command:
      - bash
      - -c
      - |
        function on_script_completion {
          # Note: This semaphore is used to terminate the TCPx side-car
          touch /semaphore/workload_terminated
        }
        trap on_script_completion EXIT
        echo "Pod on $(hostname --fqdn) is running"
        echo "Pod is assigned job index of $JOB_COMPLETION_INDEX"

        export LD_LIBRARY_PATH="/usr/local/tcpx/lib64:${LD_LIBRARY_PATH}"
        echo "Warning: Set LD_LIBRARY_PATH=$LD_LIBRARY_PATH to override the NCCL library"

        touch /lssd/hello-from-$HOSTNAME.txt
        echo "Local SSD contents (path /lssd):"; ls /lssd | sed 's/^/  /'

        # touch /cfs/hello-from-$HOSTNAME.txt
        # echo "Cluster NFS contents (path /cfs):"; ls /cfs | sed 's/^/  /'

        ls /opt/

        PAXML_DIR=/opt/paxml
        BASE_SCRIPT=${PAXML_DIR}/paxml/contrib/gpu/scripts_gpu/benchmark_gpt_multinode.sh
        PREC=bfloat16
        PERCORE_BATCH_SIZE=8
        WORKSPACE_DIR=/opt/paxml/workspace
        LOG_DIR=log_dir_synthetic_126m
        export VOCAB_PATH=None
        export XLA_PYTHON_CLIENT_MEM_FRACTION=0.85
        BASE_XLA_FLAGS=${BASE_XLA_FLAGS:-"--xla_gpu_enable_latency_hiding_scheduler=true --xla_gpu_enable_triton_gemm=false
                              --xla_gpu_simplify_all_fp_conversions --xla_gpu_enable_async_all_gather=true
                              --xla_gpu_enable_async_reduce_scatter=true  --xla_gpu_enable_highest_priority_async_stream=true
                              --xla_gpu_enable_triton_softmax_fusion=false  --xla_gpu_all_reduce_combine_threshold_bytes=51200
                              --xla_gpu_graph_level=0 --xla_gpu_enable_async_all_reduce=true"}
        export XLA_FLAGS="$BASE_XLA_FLAGS ${XLA_FLAGS:-}"

        cd $PAXML_DIR
        mkdir $LOG_DIR
        python3 -u -m paxml.main \
            --job_log_dir=$LOG_DIR \
            --fdl_config=paxml.contrib.gpu.scripts_gpu.configs.${CONFIG} \
            --fdl.FPROP_DTYPE=\"${PREC}\" \
            --fdl.PERCORE_BATCH_SIZE=$PERCORE_BATCH_SIZE \
            --enable_checkpoint_saving=False \
            --fdl.MAX_STEPS=100 \
            --alsologtostderr

        echo "Pod on $(hostname --fqdn) is exiting"
      volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
        - name: tcpx-nccl-plugin-volume
          mountPath: /usr/local/tcpx
        - name: tcpx-daemon-socket
          mountPath: /tmp
        - name: local-ssd
          mountPath: /lssd
        - name: workload-terminated-volume
          mountPath: /semaphore 
      resources:
        limits:
          nvidia.com/gpu: 8
