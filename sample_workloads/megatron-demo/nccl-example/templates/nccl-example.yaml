{{ $timestamp := now | date "2006-01-02-150405" }}
{{ $jobidentifier := randAlphaNum 8 | lower }}
apiVersion: v1
kind: Service
metadata:
  name: "nccl-job-{{$jobidentifier}}"
spec:
  clusterIP: None
  selector:
    job-name: "nccl-job-{{$jobidentifier}}"
---
{{- $root := . -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: "nccl-job-{{$jobidentifier}}"
  namespace: default
  labels:
    kueue.x-k8s.io/queue-name: a3-queue 
spec:
  suspend: true
  parallelism: {{ $root.Values.workload.nodes }}
  completions: {{ $root.Values.workload.nodes }}
  completionMode: Indexed
  template:
   metadata:
    annotations:
      kubectl.kubernetes.io/default-container: nccl
      networking.gke.io/default-interface: 'eth0'
      networking.gke.io/interfaces: |
        [
          {"interfaceName":"eth0","network":"default"},
          {"interfaceName":"eth1","network":"vpc1"},
          {"interfaceName":"eth2","network":"vpc2"},
          {"interfaceName":"eth3","network":"vpc3"},
          {"interfaceName":"eth4","network":"vpc4"}
        ]
   spec:
    hostNetwork: false
    dnsPolicy: ClusterFirstWithHostNet
    subdomain: "nccl-job-{{$jobidentifier}}"
    restartPolicy: Never
    volumes:
    - name: nvidia-install-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: lib64
      hostPath:
        path: /lib64       
    - name: tcpx-nccl-plugin-volume
      emptyDir: {}    
    - name: tcpx-daemon-socket
      hostPath:
        path: /run/tcpx
    - name: workload-terminated-volume
      emptyDir: {}        
    - name: local-ssd
      hostPath:
        path: /mnt/stateful_partition/kube-ephemeral-ssd   
    - name: cluster-file-store
      persistentVolumeClaim:
        claimName: cluster-sharedfs 
    - name: shared-memory
      emptyDir:
        medium: "Memory"
        sizeLimit: 200Gi
    initContainers:
    {{ if eq $root.Values.tcpx.enabled "true" }}
    - name: tcpx-nccl-plugin-installer
      image: "{{$root.Values.tcpx.repository}}/{{$root.Values.tcpx.pluginVersion}}"
      imagePullPolicy: Always
      volumeMounts:
      - name: tcpx-nccl-plugin-volume
        mountPath: /var/lib/tcpx
      command:
        - /bin/sh
        - -c
        - |
          /scripts/container_entry.sh install --install-nccl
    {{ end }}  
    containers:
    {{ if eq $root.Values.tcpx.enabled "true" }}
    - name: tcpd-daemon
      image: "{{$root.Values.tcpx.repository}}/{{$root.Values.tcpx.daemonVersion}}"
      imagePullPolicy: Always
      command:
      - "bash"
      - "-c"
      - |
        # /tcpgpudmarxd/build/app/tcpgpudmarxd --gpu_nic_preset a3vm --gpu_shmem_type fd --setup_param "--verbose 128 5 0" &
        /tcpgpudmarxd/build/app/tcpgpudmarxd --gpu_nic_preset a3vm --gpu_shmem_type fd  &
        while [ ! -e "/semaphore/workload_terminated" ]; do sleep 10; done
        pkill -e "^"tcpgpudmarxd || true
        sleep 15
      securityContext:
        privileged: true
      volumeMounts:
      - name: nvidia-install-dir-host
        mountPath: /usr/local/nvidia
      - name: tcpx-daemon-socket
        mountPath: /tmp
      - name: workload-terminated-volume
        mountPath: /semaphore
      env:
      - name: LD_LIBRARY_PATH
        value: /usr/local/nvidia/lib64
    {{ end }} 
    - name: nccl
      image: "{{ $root.Values.workload.image }}"
      imagePullPolicy: Always
      env:
      - name: JOB_ID
        value: "{{ $jobidentifier }}"
      - name: MASTER_ADDR
        value: "nccl-job-{{$jobidentifier}}-0.nccl-job-{{$jobidentifier}}.default.svc.cluster.local"
      - name: NNODES
        value: "{{ $root.Values.workload.nodes }}"
      - name: NCCL_TEST_BINARY
        value: "{{ $root.Values.workload.ncclTestBinary }}"
      - name: NCCL_TEST_MESSAGE_SIZE_MIN
        value: "{{ $root.Values.workload.ncclTestMessageSizeMin }}"
      - name: NCCL_TEST_MESSAGE_SIZE_MAX
        value: "{{ $root.Values.workload.ncclTestMessageSizeMax }}"
      - name: NCCL_TEST_ITERATIONS
        value: "{{ $root.Values.workload.ncclTestIterations }}"

      {{ if eq $root.Values.tcpx.enabled "true" }}

       # The following NCCL settings should likely not be adjusted:
      - name: NCCL_SOCKET_IFNAME
        value: "eth0"
      - name: NCCL_CHECK_POINTERS
        value: "0"
      - name: NCCL_DYNAMIC_CHUNK_SIZE
        value: "524288"
      - name: NCCL_P2P_NET_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_PCI_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_NVL_CHUNKSIZE
        value: "1048576"
      - name: NCCL_CROSS_NIC
        value: "0"
      - name: NCCL_ALGO
        value: "Ring"
      - name: NCCL_PROTO
        value: "Simple"
      - name: NCCL_NET_GDR_LEVEL
        value: "PIX"
      - name: NCCL_P2P_PXN_LEVEL
        value: "0"
 
      {{- range $environment_variable := $root.Values.tcpx.ncclSettings }}
      - name: {{ $environment_variable.name }}
        value: "{{ $environment_variable.value }}"
      {{- end }}
    
      # The following TCPx settings should likely not be adjusted:
      - name: NCCL_GPUDIRECTTCPX_CTRL_DEV
        value: "eth0"
      - name: NCCL_GPUDIRECTTCPX_SOCKET_IFNAME
        value: "eth1,eth2,eth3,eth4"
      - name: NCCL_GPUDIRECTTCPX_TX_BINDINGS
        value: "eth1:8-21,112-125;eth2:8-21,112-125;eth3:60-73,164-177;eth4:60-73,164-177"
      - name: NCCL_GPUDIRECTTCPX_RX_BINDINGS
        value: "eth1:22-35,126-139;eth2:22-35,126-139;eth3:74-87,178-191;eth4:74-87,178-191"
      - name: NCCL_GPUDIRECTTCPX_PROGRAM_FLOW_STEERING_WAIT_MICROS
        value: "1000000"
      - name: NCCL_GPUDIRECTTCPX_FORCE_ACK
        value: "0"
      - name: NCCL_GPUDIRECTTCPX_TX_COMPLETION_NANOSLEEP
        value: "1000"     
    
      {{- range $environment_variable := $root.Values.tcpx.tcpxSettings }}
      - name: {{ $environment_variable.name }}
        value: "{{ $environment_variable.value }}"
      {{- end }}
    
      {{ end }} 
      command:
      - bash
      - -c
      - |
        function on_script_completion {
          # Note: This semaphore is used to terminate the TCPx side-car
          touch /semaphore/workload_terminated
        }
        trap on_script_completion EXIT
        echo "Pod on $(hostname --fqdn) is running"
        echo "Pod is assigned job index of $JOB_COMPLETION_INDEX"

        mkdir /run/sshd
        /usr/sbin/sshd -p 2222
        echo "Pod has started SSH daemon"
        sleep 20 # Note: This hack allow all SSH daemon sufficient startup time

        export LD_LIBRARY_PATH="/usr/local/tcpx/lib64:${LD_LIBRARY_PATH}"
        echo "Warning: Set LD_LIBRARY_PATH=$LD_LIBRARY_PATH to override the NCCL library"

        touch /lssd/hello-from-$HOSTNAME.txt
        echo "Local SSD contents (path /lssd):"; ls /lssd | sed 's/^/  /'

        touch /cfs/hello-from-$HOSTNAME.txt
        echo "Cluster NFS contents (path /cfs):"; ls /cfs | sed 's/^/  /'

        echo "List of worker services:"
        for JOB_INDEX in $(seq 0 $((NNODES-1))); do
          WORKER="nccl-job-$JOB_ID-$JOB_INDEX.nccl-job-$JOB_ID.default.svc.cluster.local"

          echo "  Ping $WORKER"
          echo -n "  Pong "; ssh -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 2222 $WORKER hostname

          echo "Host $WORKER" >> /root/.ssh/config
          echo "  Port 2222" >> /root/.ssh/config

          echo "$WORKER slots=8" >> /etc/job-worker-services.txt
        done

        readarray -d "" nccl_environment < <(env | grep -e "^NCCL_" | sed 's/^/-x /' | tr '\n' '\0')
        echo "Detected NCCL environment:"
        for nccl_variable in "${nccl_environment[@]}"; do
          echo "  $nccl_variable"
        done

        if [ "$JOB_COMPLETION_INDEX" -eq "0" ]; then
          echo "Launching MPI job across all hosts"
          mpirun \
            --allow-run-as-root \
            -n "$((8*$NNODES))" \
            --mca orte_keep_fqdn_hostnames t \
            --hostfile /etc/job-worker-services.txt \
            --mca btl_tcp_if_include eth0 \
            --mca btl tcp,self \
            -x LD_LIBRARY_PATH \
            ${nccl_environment[@]} \
            "/third_party/nccl-tests-mpi/build/$NCCL_TEST_BINARY" \
              -g 1 -f 2 \
              -n "$NCCL_TEST_ITERATIONS" \
              -b "$NCCL_TEST_MESSAGE_SIZE_MIN" \
              -e "$NCCL_TEST_MESSAGE_SIZE_MAX"
          
          echo "Informing all worker pods that the workload has terminated:"
          for JOB_INDEX in $(seq 0 $((NNODES-1))); do
            WORKER="nccl-job-$JOB_ID-$JOB_INDEX.nccl-job-$JOB_ID.default.svc.cluster.local"
            echo -n "  Write to $WORKER:/semaphore/workload_terminated to terminate worker: "
            ssh -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 2222 $WORKER touch /semaphore/workload_terminated
            if [ "$?" -eq 0 ]; then
              echo "success"
            else
              echo "failed"
            fi
          done

        else 
          # We need the head node to tell worker when to exit
          while [ ! -e "/semaphore/workload_terminated" ]; do sleep 10; done
        fi

        echo "Pod on $(hostname --fqdn) is exiting"
      volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
        - name: tcpx-nccl-plugin-volume
          mountPath: /usr/local/tcpx
        - name: tcpx-daemon-socket
          mountPath: /tmp
        - name: local-ssd
          mountPath: /lssd
        - name: workload-terminated-volume
          mountPath: /semaphore 
        - name: cluster-file-store
          mountPath: /cfs
        - name: shared-memory
          mountPath: /dev/shm 
      resources:
        limits:
          nvidia.com/gpu: 8

