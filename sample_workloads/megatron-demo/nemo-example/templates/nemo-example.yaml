{{ $timestamp := now | date "2006-01-02-150405" }}
{{ $jobid := randAlphaNum 8 | lower }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: "nemo-job-{{$jobid}}" 
data:
  nemo-configuration.yaml: |-
{{ .Files.Get "selected-configuration.yaml" | nindent 4 }}
---
apiVersion: v1
kind: Service
metadata:
  name: "nemo-job-{{$jobid}}"
spec:
  clusterIP: None
  selector:
    job-name: "nemo-job-{{$jobid}}"
---
{{- $root := . -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: "nemo-job-{{$jobid}}"
  namespace: default
  labels:
  {{- if $root.Values.queue }}
    kueue.x-k8s.io/queue-name: "$root.Values.queue"
  {{- end }}
spec:
  {{- if $root.Values.queue }}
  suspend: true
  {{- end }}
  parallelism: {{ $root.Values.workload.nodes }}
  completions: {{ $root.Values.workload.nodes }}
  completionMode: Indexed
  template:
   metadata:
    annotations:
      kubectl.kubernetes.io/default-container: nemo 
   spec:
    hostNetwork: true
    dnsPolicy: ClusterFirstWithHostNet
    subdomain: "nemo-job-{{$jobid}}"
    restartPolicy: Never
    tolerations:
    - operator: "Exists"
      key: nvidia.com/gpu
    volumes:
    - name: nvidia-install-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: lib64
      hostPath:
        path: /lib64       
    - name: tcpx-nccl-plugin-volume
      emptyDir: {}    
    - name: tcpx-daemon-socket
      hostPath:
        path: /run/tcpx
    - name: workload-terminated-volume
      emptyDir: {}        
    - name: local-ssd
      hostPath:
        path: /mnt/stateful_partition/kube-ephemeral-ssd   
    {{- if $root.Values.volumes.nfsMountPath }}
    - name: cluster-file-store
      persistentVolumeClaim:
        claimName: cluster-sharedfs
    {{- end }}
    - name: shared-memory
      emptyDir:
        medium: "Memory"
        sizeLimit: 200Gi 
    - name: workload-configuration
      configMap:
        name: "nemo-job-{{$jobid}}"
    initContainers:
    - name: training-data-downloader
      image: gcr.io/google.com/cloudsdktool/google-cloud-cli
      volumeMounts:
      - name: local-ssd
        mountPath: "{{ $root.Values.volumes.ssdMountPath }}"
      {{- if $root.Values.volumes.nfsMountPath }}
      - name: cluster-file-store
        mountPath: "{{ $root.Values.volumes.nfsMountPath }}"
      {{- end }}
      env:
      - name: TRAINING_DATA_SOURCE
        value: "{{ $root.Values.workload.trainingDataSource }}"
      - name: NFS_MOUNT_PATH
        value: "{{ $root.Values.volumes.nfsMountPath }}"
      - name: SSD_MOUNT_PATH
        value: "{{ $root.Values.volumes.ssdMountPath }}" 
      command:
        - /bin/sh
        - -c
        - |
          echo "Caching training data from $TRAINING_DATA_SOURCE to $SSD_MOUNT_PATH/training-data"
          mkdir -p /ssd/training-data

          SECONDS=0
          gcloud storage rsync \
            --recursive \
            $TRAINING_DATA_SOURCE $SSD_MOUNT_PATH/training-data
          duration=$SECONDS
          echo "Training data downloaded to local SSD in $duration seconds."
      
    {{- if $root.Values.tcpx.enabled }}

    - name: tcpx-nccl-plugin-installer
      image: "{{$root.Values.tcpx.repository}}/{{$root.Values.tcpx.pluginVersion}}"
      imagePullPolicy: Always
      volumeMounts:
      - name: tcpx-nccl-plugin-volume
        mountPath: /var/lib/tcpx
      command:
        - /bin/sh
        - -c
        - |
          /scripts/container_entry.sh install --install-nccl

    {{- end }}  

    containers:
    {{- if $root.Values.tcpx.enabled }}
    - name: tcpd-daemon
      image: "{{$root.Values.tcpx.repository}}/{{$root.Values.tcpx.daemonVersion}}"
      imagePullPolicy: Always
      command:
      - "bash"
      - "-c"
      - |
        /tcpgpudmarxd/build/app/tcpgpudmarxd --gpu_nic_preset a3vm --gpu_shmem_type fd --setup_param "--verbose 128 5 0" &
        while [ ! -e "/semaphore/workload_terminated" ]; do sleep 10; done
        pkill -e "^"tcpgpudmarxd || true
        sleep 15
      securityContext:
        privileged: true
      volumeMounts:
      - name: nvidia-install-dir-host
        mountPath: /usr/local/nvidia
      - name: tcpx-daemon-socket
        mountPath: /tmp
      - name: workload-terminated-volume
        mountPath: /semaphore
      env:
      - name: LD_LIBRARY_PATH
        value: /usr/local/nvidia/lib64
    {{- end }} 
    - name: nemo
      image: "{{ $root.Values.workload.image }}"
      imagePullPolicy: Always
      env:
      - name: MASTER_ADDR
        value: "nemo-job-{{$jobid}}-0.nemo-job-{{$jobidentifier}}.default.svc.cluster.local"
      - name: NNODES
        value: "{{ $root.Values.workload.nodes }}"
      - name: TORCH_RUN_TARGET
        value: "{{ $root.Values.workload.torchRunTarget }}"
      - name: JOB_IDENTIFER
        value: "nemo-job-{{$jobid}}"

      # Mount paths for volumes:
      - name: NFS_MOUNT_PATH
        value: "{{ $root.Values.volumes.nfsMountPath }}"
      - name: SSD_MOUNT_PATH
        value: "{{ $root.Values.volumes.ssdMountPath }}"      
         
       # The following NCCL settings should likely not be adjusted:
      - name: NCCL_SOCKET_IFNAME
        value: "eth0"
      - name: NCCL_CHECK_POINTERS
        value: "0"
      - name: NCCL_DYNAMIC_CHUNK_SIZE
        value: "524288"
      - name: NCCL_P2P_NET_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_PCI_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_NVL_CHUNKSIZE
        value: "1048576"
      - name: NCCL_CROSS_NIC
        value: "0"
      - name: NCCL_ALGO
        value: "Ring"
      - name: NCCL_PROTO
        value: "Simple"
      - name: NCCL_NET_GDR_LEVEL
        value: "PIX"
      - name: NCCL_P2P_PXN_LEVEL
        value: "0"
 
      {{- range $environment_variable := $root.Values.tcpx.ncclSettings }}
      - name: {{ $environment_variable.name }}
        value: "{{ $environment_variable.value }}"
      {{- end }}
    
      {{- if eq $root.Values.tcpx.enabled }}

      # The following TCPx settings should likely not be adjusted:
      - name: NCCL_GPUDIRECTTCPX_CTRL_DEV
        value: "eth0"
      - name: NCCL_GPUDIRECTTCPX_SOCKET_IFNAME
        value: "eth1,eth2,eth3,eth4"
      - name: NCCL_GPUDIRECTTCPX_TX_BINDINGS
        value: "eth1:8-21,112-125;eth2:8-21,112-125;eth3:60-73,164-177;eth4:60-73,164-177"
      - name: NCCL_GPUDIRECTTCPX_RX_BINDINGS
        value: "eth1:22-35,126-139;eth2:22-35,126-139;eth3:74-87,178-191;eth4:74-87,178-191"
      - name: NCCL_GPUDIRECTTCPX_PROGRAM_FLOW_STEERING_WAIT_MICROS
        value: "1000000"
      - name: NCCL_GPUDIRECTTCPX_FORCE_ACK
        value: "0"
      - name: NCCL_GPUDIRECTTCPX_TX_COMPLETION_NANOSLEEP
        value: "1000"     
    
      {{- range $environment_variable := $root.Values.tcpx.tcpxSettings }}
      - name: {{ $environment_variable.name }}
        value: "{{ $environment_variable.value }}"
      {{- end }}
    
      {{- end }} 
      command:
      - bash
      - -c
      - |
        function on_script_completion {
          # Note: This semaphore is used to terminate the TCPx side-car
          touch /semaphore/workload_terminated
        }
        trap on_script_completion EXIT
        echo "Pod on $(hostname --fqdn) is running"
        echo "Pod is assigned job index of $JOB_COMPLETION_INDEX"

        export LD_LIBRARY_PATH="/usr/local/tcpx/lib64:${LD_LIBRARY_PATH}"
        echo "Warning: Set LD_LIBRARY_PATH=$LD_LIBRARY_PATH to override the NCCL library"

        touch $SSD_MOUNT_PATH/hello-from-$HOSTNAME.txt
        echo "Local SSD contents (path $SSD_MOUNT_PATH):"; ls $SSD_MOUNT_PATH | sed 's/^/  /'

        if [ -d "$NFS_MOUNT_PATH" ]; then
          echo "Detected $NFS_MOUNT_PATH exists and assuming an NFS mount is bound there"        

          touch $NFS_MOUNT_PATH/hello-from-$HOSTNAME.txt
          echo "NFS contents (path $NFS_MOUNT_PATH):"; ls $NFS_MOUNT_PATH | sed 's/^/  /'
        fi

        echo "NeMo configuration file:"
        cat /etc/workload-configuration/nemo-configuration.yaml | sed 's/^/| /'

        sleep 10 # <- Hack to allow some time for service to boot

        export GPUS_PER_NODE=8
        export NODE_RANK=$JOB_COMPLETION_INDEX
        export MASTER_PORT=6002
        export WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES))

        for ((LOCAL_RANK=0; LOCAL_RANK <= $((GPUS_PER_NODE - 1)); LOCAL_RANK++)); do
          RANK=$((8*$NODE_RANK + $LOCAL_RANK))

          OMP_NUM_THREADS=12 RANK=$RANK LOCAL_RANK=$LOCAL_RANK /usr/bin/python3.8 $TORCH_RUN_TARGET \
            --config-path="/etc/workload-configuration" \
            --config-name="nemo-configuration.yaml" \
            +trainer.num_nodes="$NNODES" \
            +exp_manager.version="$JOB_IDENTIFER" &
        done

        wait
        echo "Pod on $(hostname --fqdn) is exiting"
      volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
        - name: tcpx-nccl-plugin-volume
          mountPath: /usr/local/tcpx
        - name: tcpx-daemon-socket
          mountPath: /tmp
        - name: workload-terminated-volume
          mountPath: /semaphore   
        - name: workload-configuration
          mountPath: /etc/workload-configuration  
        - name: shared-memory
          mountPath: /dev/shm 
        - name: local-ssd
          mountPath: "$root.Values.volumes.ssdMountPath"
      {{- if $root.Values.volumes.nfsMountPath }}
        - name: cluster-file-store
          mountPath: "$root.Values.volumes.nfsMountPath"
      {{- end }}
      resources:
        limits:
          nvidia.com/gpu: 8
---

#        torchrun \
#          --nproc_per_node=8 \
#          --nnodes=$NNODES \
#          --rdzv_id=1002 \
#          --rdzv_backend=c10d \
#          --rdzv_endpoint=$MASTER_ADDR \
#          $TORCH_RUN_TARGET \
#          --config-path="/etc/workload-configuration" \
#          --config-name="nemo-configuration.yaml" \
#          +trainer.num_nodes="$NNODES" \
#          +exp_manager.version="$JOB_IDENTIFER"
#
           
