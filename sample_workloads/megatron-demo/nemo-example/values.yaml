queue: null # optional (must have installed Kueue and pre-provisioned 'a3-queue' queue, see previous guide steps)

workload:
  nodes: 2
  image: "$REGION-docker.pkg.dev/$PROJECT/$PREFIX:nemofw-training:23.05-py3"
  torchRunTarget: "/opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py"
  trainingDataSource: "gs://megatron-data-us/training-data/wikitext" # this folder is cached to local SSD on workload start

volumes:
  nfsMountPath: null # optional (must have pre-provisioned a FileStore and bound it to 'cluster-sharedfs' persistent volume claim, see previous guide steps)
  ssdMountPath: /ssd

networking:
  enableTcpx: "true" # required for optimal performance
  tcpxRepository: "us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx"
  tcpxDaemonVersion: "tcpgpudmarxd-dev:v2.0.9"
  tcpxPluginVersion: "nccl-plugin-gpudirecttcpx-dev:v3.1.6_2023_10_06"
 
  ncclSettings:
  - name: NCCL_DEBUG
    value: "VERSION"
 
  # The following NCCL settings are recommended (but tunable):
  - name: NCCL_MIN_NCHANNELS
    value: "8"
  - name: NCCL_MAX_NCHANNELS
    value: "8"
  - name: NCCL_SOCKET_NTHREADS
    value: "1"
  - name: NCCL_NSOCKS_PERTHREAD
    value: "4"
