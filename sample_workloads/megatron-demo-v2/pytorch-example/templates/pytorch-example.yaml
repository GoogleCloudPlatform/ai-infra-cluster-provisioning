{{ $timestamp := now | date "2006-01-02-150405" }}
{{ $jobidentifier := randAlphaNum 8 | lower }}
apiVersion: v1
kind: Service
metadata:
  name: "pytorch-job-{{$jobidentifier}}"
spec:
  clusterIP: None
  selector:
    job-name: "pytorch-job-{{$jobidentifier}}"
---
{{- $root := . -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: "pytorch-job-{{$jobidentifier}}"
  namespace: default
  labels:
    kueue.x-k8s.io/queue-name: a3-queue 
spec:
  suspend: true
  parallelism: {{ $root.Values.workload.nodes }}
  completions: {{ $root.Values.workload.nodes }}
  completionMode: Indexed
  template:
   metadata:
    annotations:
      kubectl.kubernetes.io/default-container: pytorch
   spec:
    hostNetwork: true
    dnsPolicy: ClusterFirstWithHostNet
    subdomain: "pytorch-job-{{$jobidentifier}}"
    restartPolicy: Never
    volumes:
    - name: nvidia-install-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: lib64
      hostPath:
        path: /lib64       
    - name: tcpx-nccl-plugin-volume
      emptyDir: {}    
    - name: tcpx-daemon-socket
      hostPath:
        path: /run/tcpx
    - name: workload-terminated-volume
      emptyDir: {}        
    - name: local-ssd
      hostPath:
        path: /mnt/stateful_partition/kube-ephemeral-ssd   
    - name: cluster-file-store
      persistentVolumeClaim:
        claimName: cluster-sharedfs
    initContainers:
    {{ if eq $root.Values.tcpx.enabled "true" }}
    - name: tcpx-nccl-plugin-installer
      image: "{{$root.Values.tcpx.repository}}/{{$root.Values.tcpx.pluginVersion}}"
      imagePullPolicy: Always
      volumeMounts:
      - name: tcpx-nccl-plugin-volume
        mountPath: /var/lib/tcpx
      command:
        - /bin/sh
        - -c
        - |
          /scripts/container_entry.sh install --install-nccl
    {{ end }}  
    containers:
    {{ if eq $root.Values.tcpx.enabled "true" }}
    - name: tcpd-daemon
      image: "{{$root.Values.tcpx.repository}}/{{$root.Values.tcpx.daemonVersion}}"
      imagePullPolicy: Always
      command:
      - "bash"
      - "-c"
      - |
        /tcpgpudmarxd/build/app/tcpgpudmarxd --gpu_nic_preset a3vm --gpu_shmem_type fd --setup_param "--verbose 128 5 0" &
        while [ ! -e "/semaphore/workload_terminated" ]; do sleep 10; done
        pkill -e "^"tcpgpudmarxd || true
        sleep 15
      securityContext:
        privileged: true
      volumeMounts:
      - name: nvidia-install-dir-host
        mountPath: /usr/local/nvidia
      - name: tcpx-daemon-socket
        mountPath: /tmp
      - name: workload-terminated-volume
        mountPath: /semaphore
      env:
      - name: LD_LIBRARY_PATH
        value: /usr/local/nvidia/lib64
    {{ end }} 
    - name: pytorch
      image: "{{ $root.Values.workload.image }}"
      imagePullPolicy: Always
      env:
      - name: MASTER_ADDR
        value: "pytorch-job-{{$jobidentifier}}-0.pytorch-job-{{$jobidentifier}}.default.svc.cluster.local"
      - name: NNODES
        value: "{{ $root.Values.workload.nodes }}"
      - name: TORCH_RUN_TARGET
        value: "{{ $root.Values.workload.torchRunTarget }}"

      {{ if eq $root.Values.tcpx.enabled "true" }}

       # The following NCCL settings should likely not be adjusted:
      - name: NCCL_SOCKET_IFNAME
        value: "eth0"
      - name: NCCL_CHECK_POINTERS
        value: "0"
      - name: NCCL_DYNAMIC_CHUNK_SIZE
        value: "524288"
      - name: NCCL_P2P_NET_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_PCI_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_NVL_CHUNKSIZE
        value: "1048576"
      - name: NCCL_CROSS_NIC
        value: "0"
      - name: NCCL_ALGO
        value: "Ring"
      - name: NCCL_PROTO
        value: "Simple"
      - name: NCCL_NET_GDR_LEVEL
        value: "PIX"
      - name: NCCL_P2P_PXN_LEVEL
        value: "0"
 
      {{- range $environment_variable := $root.Values.tcpx.ncclSettings }}
      - name: {{ $environment_variable.name }}
        value: "{{ $environment_variable.value }}"
      {{- end }}
    
      # The following TCPx settings should likely not be adjusted:
      - name: NCCL_GPUDIRECTTCPX_CTRL_DEV
        value: "eth0"
      - name: NCCL_GPUDIRECTTCPX_SOCKET_IFNAME
        value: "eth1,eth2,eth3,eth4"
      - name: NCCL_GPUDIRECTTCPX_TX_BINDINGS
        value: "eth1:8-21,112-125;eth2:8-21,112-125;eth3:60-73,164-177;eth4:60-73,164-177"
      - name: NCCL_GPUDIRECTTCPX_RX_BINDINGS
        value: "eth1:22-35,126-139;eth2:22-35,126-139;eth3:74-87,178-191;eth4:74-87,178-191"
      - name: NCCL_GPUDIRECTTCPX_PROGRAM_FLOW_STEERING_WAIT_MICROS
        value: "1000000"
      - name: NCCL_GPUDIRECTTCPX_FORCE_ACK
        value: "0"
      - name: NCCL_GPUDIRECTTCPX_TX_COMPLETION_NANOSLEEP
        value: "1000"     
    
      {{- range $environment_variable := $root.Values.tcpx.tcpxSettings }}
      - name: {{ $environment_variable.name }}
        value: "{{ $environment_variable.value }}"
      {{- end }}
    
      {{ end }} 
      command:
      - bash
      - -c
      - |
        function on_script_completion {
          # Note: This semaphore is used to terminate the TCPx side-car
          touch /semaphore/workload_terminated
        }
        trap on_script_completion EXIT
        echo "Pod on $(hostname --fqdn) is running"
        echo "Pod is assigned job index of $JOB_COMPLETION_INDEX"

        export LD_LIBRARY_PATH="/usr/local/tcpx/lib64:${LD_LIBRARY_PATH}"
        echo "Warning: Set LD_LIBRARY_PATH=$LD_LIBRARY_PATH to override the NCCL library"

        touch /lssd/hello-from-$HOSTNAME.txt
        echo "Local SSD contents (path /lssd):"; ls /lssd | sed 's/^/  /'

        touch /cfs/hello-from-$HOSTNAME.txt
        echo "Cluster NFS contents (path /cfs):"; ls /cfs | sed 's/^/  /'

        torchrun --nproc_per_node=8 --nnodes=$NNODES --rdzv_id=1002 --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR $TORCH_RUN_TARGET

        echo "Pod on $(hostname --fqdn) is exiting"
      volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
        - name: tcpx-nccl-plugin-volume
          mountPath: /usr/local/tcpx
        - name: tcpx-daemon-socket
          mountPath: /tmp
        - name: local-ssd
          mountPath: /lssd
        - name: workload-terminated-volume
          mountPath: /semaphore 
        - name: cluster-file-store
          mountPath: /cfs
      resources:
        limits:
          nvidia.com/gpu: 8
