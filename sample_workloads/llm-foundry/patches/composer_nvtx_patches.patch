--- trainer.py.backup	2023-12-01 22:01:48.403820973 +0000
+++ trainer.py	2023-12-01 22:05:21.951632969 +0000
@@ -57,6 +57,7 @@
                             maybe_create_object_store_from_uri, maybe_create_remote_uploader_downloader_from_uri,
                             model_eval_mode, parse_uri, reproducibility, using_torch_2)
 from composer.utils.misc import is_model_deepspeed
+import nvtx

 if is_tpu_installed():
     import torch_xla.core.xla_model as xm
@@ -2015,6 +2016,8 @@
         finished_epoch_early = False
         last_wct = datetime.datetime.now()

+        batch_range = nvtx.start_range(message="full_step_train_loop", color="green")
+
         while self.state.timestamp < self.state.max_duration:
             try:
                 if int(self.state.timestamp.batch_in_epoch) == 0:
@@ -2078,6 +2081,13 @@

                     now = datetime.datetime.now()

+                    nvtx.end_range(batch_range)
+                    torch.cuda.cudart().cudaProfilerStop()
+
+                    if batch_idx > 0 and batch_idx % 5 == 0:
+                        torch.cuda.cudart().cudaProfilerStart()
+                    batch_range = nvtx.start_range(message="full_step_train_loop", color="green")
+
                     batch_time = now - last_wct

                     total_num_samples, total_num_tokens, batch_time = self._accumulate_time_across_ranks(
@@ -2154,6 +2164,8 @@
             except BreakEpochException:
                 log.info(f'Skipping the rest of Epoch {int(self.state.timestamp.epoch)}')

+        torch.cuda.cudart().cudaProfilerStop()
+
         # Log final time values
         self.logger.log_metrics({
             'time/epoch': self.state.timestamp.epoch.value,
@@ -2204,7 +2216,8 @@
                 )

         self.engine.run_event(Event.EVAL_AFTER_ALL)
-
+
+    @nvtx.annotate(message="_train_batch", color="blue")
     def _train_batch(self, use_grad_scaling: bool) -> Dict[str, torch.Tensor]:
         """Compute loss by training on a full batch of data.