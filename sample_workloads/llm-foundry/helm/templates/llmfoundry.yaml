{{- $requiredVar := .Values.cluster.nNodes | required ".Values.cluster.nNodes is required" -}}
{{- $requiredVar := .Values.cluster.nodePool | required ".Values.cluster.nodePool is required" -}}
{{- $requiredVar := .Values.network.ncclIfnames | required ".Values.ncclIfnames is required" -}}
{{- $requiredVar := .Values.workload.gcsBucket | required ".Values.gcsBucket is required" -}}
{{- $requiredVar := .Values.workload.jobTimestamp | required ".Values.jobTimestamp is required" -}}
{{- $requiredVar := .Values.workload.experimentDir | required ".Values.experimentDir is required" -}}
{{- $requiredVar := .Values.workload.dataDir| required ".Values.dataDir is required" -}}
{{- $requiredVar := .Values.workload.image | required ".Values.image is required" -}}
apiVersion: v1
kind: Service
metadata:
  name: "llmfoundry-leader-{{$.Release.Name}}"
spec:
  selector:
    name: "llmfoundry-leader-{{$.Release.Name}}"
  clusterIP: None
  ports:
  - name: llmfoundry-leader
    port: 6002
---
{{$node_count := .Values.cluster.nNodes | int}}
# This needs to be updated to allow uneven distribution of nodes to SBs
{{- $root := . -}}
{{range $node_index, $element := until $node_count}}
apiVersion: v1
kind: Pod
metadata:
  name: llmfoundry-{{$.Release.Name}}-pod{{$node_index}}
  {{if eq $node_index 0}}
  labels:
    name: llmfoundry-leader-{{$.Release.Name}}
  {{end}}
spec:
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
  hostname: llmfoundry-pod{{$node_index}}
  subdomain: llmfoundry-{{$.Release.Name}}
  serviceAccountName: "default"
  restartPolicy: Never
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: cloud.google.com/gke-accelerator
            operator: Exists
          - key: cloud.google.com/gke-nodepool
            operator: In
            values: [{{$.Values.cluster.nodePool}}]
  tolerations:
  - operator: "Exists"
    key: nvidia.com/gpu
  volumes:
  - name: nvidia-install-dir-host
    hostPath:
      path: /home/kubernetes/bin/nvidia/lib64
  - name: tcpd-socket
    hostPath:
      path: /run/tcpx
  - name: shared-memory
    emptyDir:
      medium: "Memory"
      sizeLimit: 200Gi
  - name: workload-terminated-volume
    emptyDir: {}
  - name: tcpx-nccl-plugin-volume
    emptyDir: {}
  {{if eq $root.Values.network.useTcpx "yes"}}
  initContainers:
  - name: tcpx-nccl-plugin-installer
    image: {{$root.Values.network.ncclPlugin}}
    imagePullPolicy: Always
    volumeMounts:
    - name: tcpx-nccl-plugin-volume
      mountPath: /var/lib/tcpx
    resources:
      requests:
        cpu: 150m
    command:
      - /bin/sh
      - -c
      - |
        /scripts/container_entry.sh install --install-nccl
  {{end}}
  containers:
  {{if eq $root.Values.network.useTcpx "yes"}}
  - name: tcpd-daemon
    image: {{$root.Values.network.rxdmContainer}}
    imagePullPolicy: Always
    command:
    - "bash"
    - "-c"
    - |
      /tcpgpudmarxd/build/app/tcpgpudmarxd --gpu_nic_preset a3vm --gpu_shmem_type fd --setup_param "--verbose 128 5 0" &
      while [ ! -e "/usr/share/llmfoundry/workload_terminated" ]; do sleep 10; done
    securityContext:
      privileged: true
    volumeMounts:
    - name: nvidia-install-dir-host
      mountPath: /usr/local/nvidia/lib64
    - name: tcpd-socket
      mountPath: /tmp
    - name: workload-terminated-volume
      mountPath: /usr/share/llmfoundry
    env:
    - name: LD_LIBRARY_PATH
      value: /usr/local/nvidia/lib64
  {{end}}
  - name: llmfoundry
    image: {{$root.Values.workload.image}}
    imagePullPolicy: Always
    command: [ "composer" ]
    args: [ "train/train.py", "train/yamls/pretrain/{{$root.Values.workload.modelName}}.yaml", "data_local=my-copy-c4", "train_loader.dataset.split=train_small", "eval_loader.dataset.split=val_small", "max_duration=10ba", "eval_interval=0", "save_folder={{$root.Values.workload.modelName}}"]
    securityContext:
      privileged: true
      capabilities:
        add:
          - SYS_ADMIN
          - SYS_PTRACE
          - IPC_LOCK
    env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      - name: NODE_IP
        valueFrom:
          fieldRef:
            fieldPath: status.hostIP
      - name: LD_LIBRARY_PATH
        value: "/usr/local/nvidia/lib64"
      - name: MASTER_ADDR
        value: "llmfoundry-leader-{{$.Release.Name}}"
      - name: MASTER_PORT
        value: 6002
      - name: NCCL_SOCKET_IFNAME
        value: "{{$root.Values.network.ncclIfnames}}"
      - name: NNODES
        value: "{{$node_count}}"
      - name: WORLD_SIZE
        value: "{{mul $node_count 8}}"
      - name: NODE_RANK
        value: "{{ $node_index }}"
      - name: USE_TCPX
        value: "{{$root.Values.network.useTcpx}}"
      - name: TCPX_FORCE_ACK
        value: "{{$root.Values.network.tcpxForceAck}}"
      - name: DISABLE_PMTU
        value: "{{$root.Values.network.disablePmtu}}"
      - name: CPU_PINNING_MODE
        value: "{{$root.Values.network.cpuPinningMode}}"
      - name: GCS_BUCKET
        value: "{{$root.Values.workload.gcsBucket}}"
      - name: EXPERIMENT_ROOT_DIR
        value: "{{$root.Values.workload.experimentDir}}"
      - name: DATA_DIR
        value: "{{$root.Values.workload.dataDir}}"
      - name: BATCH_SIZE
        value: "{{$root.Values.workload.batchSize}}"
      - name: MICRO_BATCH_SIZE
        value: "{{$root.Values.workload.microBatchSize}}"
      - name: MODEL_NAME
        value: "{{$root.Values.workload.modelName}}"
    volumeMounts:
      - name: nvidia-install-dir-host
        mountPath: /usr/local/nvidia/lib64
      - name: tcpx-nccl-plugin-volume
        mountPath: /usr/local/tcpx
      - name: tcpd-socket
        mountPath: /tmp
      - name: shared-memory
        mountPath: /dev/shm
      - name: workload-terminated-volume
        mountPath: /usr/share/llmfoundry
    resources:
      limits:
        nvidia.com/gpu: !!int 8
---
{{end}}
