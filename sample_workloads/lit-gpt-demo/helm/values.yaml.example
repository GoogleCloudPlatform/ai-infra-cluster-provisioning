cluster:
  nNodes: <int: add number of nodes here>
  nodePool: <str: add nodepool name here (no need to add quotes)>
network:
  useTcpx: <str: either "yes" or "no">
  ncclIfnames: 'eth0'
  ncclPlugin: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/nccl-plugin-gpudirecttcpx-dev:v3.1.6_2023_10_06
  rxdmContainer: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/tcpgpudmarxd-dev:v2.0.9
  disablePmtu: "yes"


workload:
  jobTimestamp: <int: add a timestamp here or unique identifier>
  gcsExperimentBucket: <str: your gcs bucket where experiment logs should go>
  experimentDir: <str: root gcs directory of experiment>
  gcsDataBucket: <str: your gcs bucket where data is located>
  dataDir: <str: location with gcs bucket of where train.bin and val.bin are located>
  image: us-central1-docker.pkg.dev/<YOUR PROJECT ID>/<ARTIFACT REGISTRY NAME>/litgpt-full:<ADD TAG HERE>
  configDirInBucket: null
  batchSize: 6
  microBatchSize: 6
  modelName: Llama-2-70b-hf
  warmupIters: 10
  maxIters: 1000